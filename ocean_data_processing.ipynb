{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ocean Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT REQUIRED MODULES\n",
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import gsw # Gibbs-SeaWater (GSW) Oceanographic Toolbox "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OMG ALAMO Floats (F9250 & F9313)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code processes the data recorded by two OMG ALAMO floats positioned within Disko Bay, F9250 & F9313 (https://podaac.jpl.nasa.gov/dataset/OMG_L1_FLOAT_ALAMO). The data are provided as JSON files - to begin, the number of JSON files provided within each directory is printed. The JSON files are then processed in order to sample oceanic variables at a specified depth. For the purposes of this study, oceanic variables were sampled at a depth of 240M within Disko Bay. Variables such as temperature, pressure and depth are extracted, with the date, longitude and latitude of each dive also recorded from the 'DiveStart' metadata. It should be noted that depth is not measured directly and is therefore calculated from pressure, using a package from the Gibbs-SeaWater (GSW) Oceanographic Toolbox (https://www.teos-10.org/software.htm#1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of JSON files in ALAMO_F9250 directory: 90\n",
      "Number of JSON files in ALAMO_F9313 directory: 94\n"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate the number of JSON files within a specified directory. \n",
    "def count_json_files(directory):\n",
    "    file_count = 0\n",
    "    for (dir_path, _, file_names) in os.walk(os.path.normpath(directory)):\n",
    "        for file in file_names:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_count += 1\n",
    "    return file_count\n",
    "\n",
    "ALAMO_F9250_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/OMG_ALAMO_F9250/'\n",
    "ALAMO_F9313_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/OMG_ALAMO_F9313/'\n",
    "print(f\"Number of JSON files in ALAMO_F9250 directory: {count_json_files(ALAMO_F9250_directory)}\")\n",
    "print(f\"Number of JSON files in ALAMO_F9313 directory: {count_json_files(ALAMO_F9313_directory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled data saved to R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/OMG_ALAMO_F9250/ALAMO_F9250_240m.csv\n"
     ]
    }
   ],
   "source": [
    "# SPECIFY THE INPUT DIRECTORY, DESIRED SAMPLING DEPTH AND OUTPUT FILENAME.\n",
    "directory = ALAMO_F9250_directory\n",
    "desired_depth = 240\n",
    "output_filename = 'ALAMO_F9250_240m.csv'\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data.\n",
    "combined_df = pd.DataFrame(columns=['date', 'lon', 'lat','temperature', 'salinity', 'pressure', 'depth'])\n",
    "\n",
    "# Within the specified directory, open each JSON file in read mode. \n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        json_path = os.path.join(directory, filename)\n",
    "        with open(json_path, 'r') as openfile:\n",
    "            CTD_data = json.load(openfile)\n",
    "        CTD_data_dict = CTD_data[0]\n",
    "        \n",
    "        # Extract the longitude and latitude from the 'DiveStart'. If the JSON file doesn't have any lon or lat values, the file is skipped. \n",
    "        lat = CTD_data_dict.get('dives', [])[0].get('trajectory', {}).get('gps', [])[0].get('lat', None)\n",
    "        lon = CTD_data_dict.get('dives', [])[0].get('trajectory', {}).get('gps', [])[0].get('lon', None)\n",
    "        if lat == 0 or lon == 0:\n",
    "            continue\n",
    "\n",
    "        # Define the temperature, salinity and pressure keys within the CTD data dictionary.\n",
    "        temperature = CTD_data_dict.get('dives', [])[0].get('science', {}).get('ascending', {}).get('binned', {}).get('temperature', [])\n",
    "        salinity = CTD_data_dict.get('dives', [])[0].get('science', {}).get('ascending', {}).get('binned', {}).get('salinity', [])\n",
    "        pressure = np.array(CTD_data_dict.get('dives', [])[0].get('science', {}).get('ascending', {}).get('binned', {}).get('pressure', []))\n",
    "\n",
    "        # Check that the temperature and pressure variables are the same length (i.e. no missing values)\n",
    "        if len(temperature) == len(pressure) > 0:\n",
    "\n",
    "            # Calculate depth based on pressure, using the gsw package.\n",
    "            latitude = 69.2\n",
    "            depth = -1 * gsw.z_from_p(pressure, latitude)\n",
    "\n",
    "            # Create a dictionary with the keys for each desired variable. Convert this dictionary to a pandas DataFrame.\n",
    "            data_dict = {'temperature': temperature, 'salinity': salinity, 'pressure': pressure, 'depth': depth,\n",
    "                         'lon': [CTD_data_dict.get('dives', [])[0].get('trajectory', {}).get('gps', [])[0].get('lon', None)] * len(temperature),\n",
    "                         'lat': [CTD_data_dict.get('dives', [])[0].get('trajectory', {}).get('gps', [])[0].get('lat', None)] * len(temperature),\n",
    "                         'date': [CTD_data_dict.get('dives', [])[0].get('trajectory', {}).get('gps', [])[0].get('datetime', None)] * len(temperature)}\n",
    "            df = pd.DataFrame(data_dict)\n",
    "\n",
    "            # Extract the row of data sampled closest to the specified desired depth.\n",
    "            closest_row = df.iloc[(df['depth'] - desired_depth).abs().argsort()[:1]]\n",
    "            combined_df = pd.concat([combined_df, closest_row], ignore_index=True)\n",
    "\n",
    "# Convert the 'date' column to the desired format\n",
    "combined_df['date'] = pd.to_datetime(combined_df['date']).dt.strftime('%d/%m/%Y')\n",
    "output_csv = os.path.join(directory, output_filename)  # Use the predefined variable for the output filename\n",
    "\n",
    "# Save the combined DataFrame to an output CSV. \n",
    "combined_df.to_csv(output_csv, index=False)\n",
    "print(f\"Sampled data saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OMG APEX Float (F9184)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greenland Ecosystem Monitoring "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "velocity_download",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
