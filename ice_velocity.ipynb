{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ice Surface Velocity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT REQUIRED MODULES [velocity_download environment]\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "from shapely.geometry import box\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ITS LIVE (2018-2023)\n",
    "The first block of this script will download all available 6-day and 12-day velocity pairs between 2018 and 2023 using the ITS LIVE Zarr Datacubes (https://its-live.jpl.nasa.gov/). Sequential identifiers are used to ensure that files with duplicate filenames are all saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of datacubes intersecting the specified study region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of datacubes intersecting the study region is: 1\n"
     ]
    }
   ],
   "source": [
    "# DEFINE THE OUTPUT DIRECTORY TO WHICH THE 6-DAY PAIRS SHOULD BE DOWNLOADED\n",
    "output_dir = 'C:/Users/s2451953/Desktop/TEST/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "# DEFINE THE STUDY REGION FOR THE 6-DAY PAIRS (JAKOBSHAVN ISBRAE)\n",
    "bounds = -190000, -2290000, -145000, -2260000 # xmin, ymin, xmax, ymax\n",
    "aoi_3413 = gpd.GeoDataFrame(geometry=[box(*(bounds))], crs=3413)\n",
    "\n",
    "# SEARCH AND PRINT THE NUMBER OF DATACUBES INTERSECTING THE STUDY REGION\n",
    "url = 'https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json'\n",
    "response = urlopen(url)\n",
    "data_json = json.loads(response.read())\n",
    "df = pd.read_json('https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json')\n",
    "gdf = gpd.GeoDataFrame.from_features(df[\"features\"])\n",
    "gdf = gdf[gdf['epsg']==3413]\n",
    "gdf = gdf.set_crs(4326).to_crs(3413)\n",
    "gdf_intsct = gdf[gdf.intersects(aoi_3413.geometry.values[0])]\n",
    "number_of_datacubes = len(gdf_intsct)\n",
    "print('The number of datacubes intersecting the study region is:', number_of_datacubes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the files to be downloaded, specifying only 6-day and 12-day pairs should be included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset from tile ITS_LIVE_vel_EPSG3413_G0120_X-150000_Y-2250000\n",
      "Appending to list.\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA STRUCTURES TO PROCESS THE ZARR FILES\n",
    "url_zarr_list = gdf_intsct.zarr_url.values\n",
    "xds_list = []\n",
    "fn_list = []\n",
    "filename_counts = {}\n",
    "\n",
    "for url_zarr in url_zarr_list:\n",
    "    filename = os.path.basename(url_zarr).split('.')[0]\n",
    "    fn_list.append(filename)\n",
    "    print(f'Preparing dataset from tile {filename}')\n",
    "    xds = xr.open_zarr(url_zarr) # Open the ZARR file in xarray\n",
    "    if filename in filename_counts: # Modify the filename to include a sequential identifier\n",
    "        filename_counts[filename] += 1\n",
    "        identifier = f'T{filename_counts[filename]}' # Append 'T' and the count\n",
    "    else:\n",
    "        filename_counts[filename] = 1\n",
    "        identifier = ''  # No identifier added if it is the first occurrence of the filename\n",
    "    filename = f'{filename}{identifier}'\n",
    "    xds = xds[['satellite_img1','satellite_img2','acquisition_date_img1','acquisition_date_img2', 'date_dt','v']]\n",
    "    xds = xds.rio.write_crs('epsg:3413')\n",
    "    xds = xds.rio.clip(aoi_3413.geometry) # Clip to the study region\n",
    "    xds = xds.where(((xds.date_dt.dt.days == 6) | (xds.date_dt.dt.days == 12)).compute(), drop=True) # Include 6-day and 12-day Pairs\n",
    "    xds = xds.where(((xds.satellite_img1 == '1A') | (xds.satellite_img1 == '1B')).compute(), drop=True)\n",
    "    var_obj_list = ['satellite_img1', 'satellite_img2']\n",
    "    for var in var_obj_list:\n",
    "        xds[var] = xds[var].astype('str')\n",
    "    xds = xds.chunk()\n",
    "    xds_list.append(xds)\n",
    "    print(f'Appending to list.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the specified 6-day and 12-day tiff files to the defined output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing dataset ITS_LIVE_vel_EPSG3413_G0120_X-150000_Y-2250000. This may take a while...\n",
      "Computed dataset. Saving images...\n",
      "Saved images.\n"
     ]
    }
   ],
   "source": [
    "# LOOP THROUGH THE XDS_LIST AND FN_LIST TOGETHER\n",
    "for xds, filename in zip(xds_list, fn_list):\n",
    "    if not os.path.exists(f'{output_dir}/{filename}'):\n",
    "        os.mkdir(f'{output_dir}/{filename}')\n",
    "    print(f'Computing dataset {filename}. This may take a while...')\n",
    "    with xds.compute() as xds:\n",
    "        print(f'Computed dataset. Saving images...')\n",
    "        filename_counts = {}\n",
    "        for _, x in xds.groupby('mid_date'):\n",
    "            outname = f'{x.acquisition_date_img1.dt.date.values[0]}_{x.acquisition_date_img2.dt.date.values[0]}_S{x.satellite_img1.values[0]}_S{x.satellite_img2.values[0]}'\n",
    "            if outname in filename_counts:\n",
    "                filename_counts[outname] += 1\n",
    "                identifier = f'_T{filename_counts[outname]}'   \n",
    "            else:\n",
    "                filename_counts[outname] = 1\n",
    "                identifier = '' \n",
    "            if identifier == '_T1':\n",
    "                identifier = ''\n",
    "            outfpath = f'{output_dir}/{filename}/{outname}{identifier}.tif'\n",
    "            x.v.rio.to_raster(outfpath, compress='ZSTD', predictor=3, zlevel=1)\n",
    "        print('Saved images.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete any tiff files within the output directory that don't fall within a specified date range (2018-2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deletion of tiff files outside the desired study period complete.\n"
     ]
    }
   ],
   "source": [
    "# DELETE ANY FILES WHICH HAVE A 'MID-DATE' OUTSIDE OF THE DESIRED DATE RANGE\n",
    "start_date = datetime(2018, 1, 1)\n",
    "end_date = datetime(2022, 12, 31)\n",
    "filtered_files = []\n",
    "directory = 'C:/Users/s2451953/Desktop/TEST/ITS_LIVE_vel_EPSG3413_G0120_X-150000_Y-2250000'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.tif'):\n",
    "        parts = filename.split('_')\n",
    "        file_end_date = datetime.strptime(parts[1], '%Y-%m-%d')\n",
    "        if file_end_date < start_date or file_end_date > end_date:\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            os.remove(file_path)\n",
    "print(\"Deletion of tiff files outside the desired study period complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second section of this script will sample each 6-day and 12-day velocity tiff file at specified sampling points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT REQUIRED MODULES [velocity_merge environment]\n",
    "import os\n",
    "import csv\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input and output file directories and provide the coordinates of the desired sampling locations. The code will sample velocity at these sampling locations and save the data to a CSV. For each data point, the filename of the tiff will be provided, with the start date, end date and mid date also shown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extracted and saved to 'C:/Users/s2451953/Desktop/TEST/ITS_LIVE_SAMPLED.csv'.\n"
     ]
    }
   ],
   "source": [
    "# DEFINE THE REGION AND PROVIDE THE FILEPATH TO THE GEOTIFFS\n",
    "folder_path = 'C:/Users/s2451953/Desktop/TEST/ITS_LIVE_vel_EPSG3413_G0120_X-150000_Y-2250000'\n",
    "\n",
    "# DEFINE THE OUTPUT CSV FILE LOCATION AND NAME\n",
    "csv_file = 'C:/Users/s2451953/Desktop/TEST/ITS_LIVE_SAMPLED.csv'\n",
    "\n",
    "# LIST THE X AND Y CO-ORDINATES AT WHICH VELOCITY SHOULD BE EXTRACTED\n",
    "coordinates = [(-180091.9495, -2278118.175),  \n",
    "    (-176595.9169, -2281533.351),\n",
    "    (-171733.1094, -2280782.72),\n",
    "    (-166897.6833, -2279795.733),\n",
    "    (-161907.8652, -2279370.56),\n",
    "    (-157584.877, -2279370.56),\n",
    "    (-153231.406, -2279370.56),]\n",
    "\n",
    "# CREATE A CSV THAT WILL SAMPLE VELOCITY FROM EACH POINT, PROVIDING DETAILS OF THE CORRESPONDING FILENAME, START DATE, END DATE AND MID DATE\n",
    "csv_headers = ['Filename', 'start_date', 'end_date', 'mid_date']\n",
    "for i, (x, y) in enumerate(coordinates, start=1):\n",
    "    csv_headers.append(f'POINT_{i}_VELOCITY')  # Custom column headers for each point's velocity\n",
    "with open(csv_file, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(csv_headers)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.tif') or filename.endswith('.tiff'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            date_parts = filename.split('_')\n",
    "            start_date = date_parts[0]\n",
    "            end_date = date_parts[1]\n",
    "            start_date_obj = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "            end_date_obj = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "            mid_date_obj = start_date_obj + (end_date_obj - start_date_obj) / 2\n",
    "            mid_date = mid_date_obj.strftime(\"%Y/%m/%d\")\n",
    "            dataset = gdal.Open(file_path, gdal.GA_ReadOnly)\n",
    "            if dataset is not None:\n",
    "                values = [filename, start_date, end_date, mid_date]\n",
    "                for x, y in coordinates:\n",
    "                    geotransform = dataset.GetGeoTransform()\n",
    "                    x_pixel = int((x - geotransform[0]) / geotransform[1])\n",
    "                    y_pixel = int((y - geotransform[3]) / geotransform[5])\n",
    "                    band = dataset.GetRasterBand(1)\n",
    "                    value = band.ReadAsArray(x_pixel, y_pixel, 1, 1)[0][0]\n",
    "                    if np.isnan(value):\n",
    "                        value = ''\n",
    "                    values.append(value)\n",
    "                csv_writer.writerow(values)\n",
    "                dataset = None\n",
    "\n",
    "print(f\"Data extracted and saved to '{csv_file}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "velocity_download",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
