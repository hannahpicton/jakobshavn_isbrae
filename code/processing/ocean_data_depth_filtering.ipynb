{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ocean Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT REQUIRED MODULES\n",
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import gsw # Gibbs-SeaWater (GSW) Oceanographic Toolbox \n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OMG ALAMO Floats (F9250 & F9313)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code processes the data recorded by two OMG ALAMO floats positioned within Disko Bay, F9250 & F9313 (https://podaac.jpl.nasa.gov/dataset/OMG_L1_FLOAT_ALAMO). The data are provided as JSON files - to begin, the number of JSON files provided within each directory is printed. The JSON files are then processed in order to sample oceanic variables at a specified depth. For the purposes of this study, oceanic variables were sampled at a depth of 240M within Disko Bay. Variables such as temperature, pressure and depth are extracted, with the date, longitude and latitude of each dive also recorded from the 'DiveStart' metadata. It should be noted that depth is not measured directly and is therefore calculated from pressure, using a package from the Gibbs-SeaWater (GSW) Oceanographic Toolbox (https://www.teos-10.org/software.htm#1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of JSON files in ALAMO_F9250 directory: 90\n",
      "Number of JSON files in ALAMO_F9313 directory: 94\n"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate the number of JSON files within a specified directory. \n",
    "def count_json_files(directory):\n",
    "    file_count = 0\n",
    "    for (dir_path, _, file_names) in os.walk(os.path.normpath(directory)):\n",
    "        for file in file_names:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_count += 1\n",
    "    return file_count\n",
    "\n",
    "ALAMO_F9250_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_alamo_f9250/'\n",
    "ALAMO_F9313_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_alamo_f9313/'\n",
    "print(f\"Number of JSON files in ALAMO_F9250 directory: {count_json_files(ALAMO_F9250_directory)}\")\n",
    "print(f\"Number of JSON files in ALAMO_F9313 directory: {count_json_files(ALAMO_F9313_directory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_alamo_f9250/ALAMO_F9250_240m.csv\n"
     ]
    }
   ],
   "source": [
    "# Specify the input directory, desired sampling depth and output filename.\n",
    "directory = ALAMO_F9250_directory\n",
    "desired_depth = 240\n",
    "output_filename = 'ALAMO_F9250_240m.csv'\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data.\n",
    "combined_df = pd.DataFrame(columns=['date', 'lon', 'lat','temperature', 'salinity', 'pressure', 'depth'])\n",
    "\n",
    "# Within the specified directory, open each JSON file in read mode. \n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        json_path = os.path.join(directory, filename)\n",
    "        with open(json_path, 'r') as openfile:\n",
    "            CTD_data = json.load(openfile)\n",
    "        CTD_data_dict = CTD_data[0]\n",
    "        \n",
    "        # Extract the longitude and latitude from the 'DiveStart'. If the JSON file doesn't have any lon or lat values, the file is skipped. \n",
    "        lat = CTD_data_dict.get('dives', [])[0].get('trajectory', {}).get('gps', [])[0].get('lat', None)\n",
    "        lon = CTD_data_dict.get('dives', [])[0].get('trajectory', {}).get('gps', [])[0].get('lon', None)\n",
    "        if lat == 0 or lon == 0:\n",
    "            continue\n",
    "\n",
    "        # Define the temperature, salinity and pressure keys within the CTD data dictionary.\n",
    "        temperature = CTD_data_dict.get('dives', [])[0].get('science', {}).get('ascending', {}).get('binned', {}).get('temperature', [])\n",
    "        salinity = CTD_data_dict.get('dives', [])[0].get('science', {}).get('ascending', {}).get('binned', {}).get('salinity', [])\n",
    "        pressure = np.array(CTD_data_dict.get('dives', [])[0].get('science', {}).get('ascending', {}).get('binned', {}).get('pressure', []))\n",
    "\n",
    "        # Check that the temperature and pressure variables are the same length (i.e. no missing values)\n",
    "        if len(temperature) == len(pressure) > 0:\n",
    "\n",
    "            # Calculate depth based on pressure, using the gsw package.\n",
    "            latitude = 69.2\n",
    "            depth = -1 * gsw.z_from_p(pressure, latitude)\n",
    "\n",
    "            # Create a dictionary with the keys for each desired variable. Convert this dictionary to a pandas DataFrame.\n",
    "            data_dict = {'temperature': temperature, 'salinity': salinity, 'pressure': pressure, 'depth': depth,\n",
    "                         'lon': [CTD_data_dict.get('dives', [])[0].get('trajectory', {}).get('gps', [])[0].get('lon', None)] * len(temperature),\n",
    "                         'lat': [CTD_data_dict.get('dives', [])[0].get('trajectory', {}).get('gps', [])[0].get('lat', None)] * len(temperature),\n",
    "                         'date': [CTD_data_dict.get('dives', [])[0].get('trajectory', {}).get('gps', [])[0].get('datetime', None)] * len(temperature)}\n",
    "            df = pd.DataFrame(data_dict)\n",
    "\n",
    "            # Extract the row of data sampled closest to the specified desired depth.\n",
    "            closest_row = df.iloc[(df['depth'] - desired_depth).abs().argsort()[:1]]\n",
    "            combined_df = pd.concat([combined_df, closest_row], ignore_index=True)\n",
    "\n",
    "# Convert the 'date' column to the desired format\n",
    "combined_df['date'] = pd.to_datetime(combined_df['date']).dt.strftime('%d/%m/%Y')\n",
    "output_csv = os.path.join(directory, output_filename)  # Use the predefined variable for the output filename\n",
    "\n",
    "# Save the combined DataFrame to an output CSV. \n",
    "combined_df.to_csv(output_csv, index=False)\n",
    "print(f\"Combined data saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OMG APEX Float (F9184)\n",
    "The following code processes the data recorded by OMG APEX float F9184 (https://podaac.jpl.nasa.gov/dataset/OMG_L1_FLOAT_APEX). This float was deployed in Disko Bay in September 2020. This profile was autonomous, with data from each dive saved as to a 'science log' csv. To begin, the science logs are filtered, with profiles only kept if pressure, temperature and salinity data were provided. Using the filtered CSVs, depth is then calculated based upon pressure, using the Gibbs Sea Water Oceanographic Toolbox module. The longitude and latitude of each profile is extracted using the first GPS coordinate recorded (i.e. the start location). Each profile is then sampled at the desired depth (240m), with a threshold of 1m used. If there are multiple values within 1 m, the data provided at the closest depth to that desired is extracted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CSV files have been filtered and saved.\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the science log csvs.\n",
    "input_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/science_logs/'\n",
    "\n",
    "# Define the directory to output the filtered csvs, containing temperature, pressure and salinity data.\n",
    "output_folder = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/filtered_science_logs/'\n",
    "\n",
    "# Rows to export from defined based on the 'message column (export those with at least temperature, pressure and salinity data.)\n",
    "target_strings = ['LGR_CP_PTS', 'LGR_CP_PTSC', 'LGR_CP_PTSCI']\n",
    "\n",
    "# Define the columns to label in each output csv.\n",
    "desired_columns = ['MESSAGE', 'TIME', 'PRESSURE', 'TEMP', 'SALINITY', 'CONDUCTIVITY', 'INTERNAL_TEMP']\n",
    "\n",
    "# LOOP THROUGH CSV FILES IN THE INPUT DIRECTORY \n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        input_filepath = os.path.join(input_directory, filename)\n",
    "        output_filename = f'filtered_{filename}'\n",
    "        output_filepath = os.path.join(output_folder, output_filename)\n",
    "        with open(input_filepath, 'r') as infile, open(output_filepath, 'w', newline='') as outfile:\n",
    "            reader = csv.reader(infile)\n",
    "            writer = csv.writer(outfile)\n",
    "            # Write the new column names\n",
    "            writer.writerow(desired_columns)\n",
    "            # Iterate through rows and write only if the first column matches the target strings\n",
    "            for row in reader:\n",
    "                if row and row[0] in target_strings:\n",
    "                    # Extract the values in the desired order\n",
    "                    filtered_row = [row[0], row[1], row[2], row[3], row[4], row[5], row[6]]\n",
    "                    writer.writerow(filtered_row)\n",
    "\n",
    "print(\"The CSV files have been filtered and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Depth has been calculated based on pressure. The updated CSVs have been saved.\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the filtered science log csvs.\n",
    "input_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/filtered_science_logs/'\n",
    "\n",
    "# Define the directory to output the filtered csvs, containing temperature, pressure and salinity data, with depth added.\n",
    "output_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/filtered_science_logs_depth/'\n",
    "\n",
    "# Latitude for Disko Bay (approximately 69.2 degrees North)\n",
    "latitude = 69.2\n",
    "\n",
    "# Loop through filtered CSV files in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.startswith('filtered_') and filename.endswith('.csv'):\n",
    "        input_filepath = os.path.join(input_directory, filename)\n",
    "        output_filepath = os.path.join(output_directory, filename)\n",
    "        with open(input_filepath, 'r') as infile, open(output_filepath, 'w', newline='') as outfile:\n",
    "            reader = csv.reader(infile)\n",
    "            writer = csv.writer(outfile)\n",
    "            header = next(reader)\n",
    "            header.append('DEPTH')\n",
    "            writer.writerow(header)\n",
    "            for row in reader:\n",
    "                message, time_str, pressure, temp, salinity, conductivity, internal_temp = row[:7]\n",
    "                pressure_dbar = float(pressure)  # Pressure should be in dbar\n",
    "                depth = gsw.z_from_p(pressure_dbar, latitude)\n",
    "                depth = -1 * depth\n",
    "                row.append(depth)\n",
    "                writer.writerow(row)\n",
    "\n",
    "print(\" Depth has been calculated based on pressure. The updated CSVs have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longitude and latitude locations of each profile have been extracted and saved.\n"
     ]
    }
   ],
   "source": [
    "# Using the original science logs, extract the longitude and latitude of each profile. This is extracted as the first GPS coordinate recorded (i.e. start location).\n",
    "\n",
    "original_folder_path = \"R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/science_logs/\" # Path to the folder containing the original CSV files\n",
    "output_file = \"R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/f9184_lon_lat.csv\" # Path for a csv containing the extracted longitude and latitude of each profile.\n",
    "extracted_rows = [] # Create an empty list to store the extracted rows\n",
    "output_column_headings = [\"FILENAME\", \"TIME\", \"LAT\", \"LON\"] # Define the column headings for the output CSV\n",
    "\n",
    "# Loop through each CSV file in the original folder\n",
    "for filename in os.listdir(original_folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        csv_file_path = os.path.join(original_folder_path, filename)\n",
    "        \n",
    "        with open(csv_file_path, 'r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            gps_found = False\n",
    "            \n",
    "            # Loop through rows of the CSV file\n",
    "            for row in csv_reader:\n",
    "                if row and row[0].strip().lower() == 'gps':\n",
    "                    if not gps_found:\n",
    "                        # Extract latitude and longitude data\n",
    "                        extracted_row = [filename] + row[1:-1]\n",
    "                        extracted_rows.append(extracted_row)\n",
    "                        gps_found = True\n",
    "                        break  # Stop searching after finding GPS data\n",
    "\n",
    "# Write the extracted data to the output CSV file\n",
    "with open(output_file, 'w', newline='') as out_csv:\n",
    "    csv_writer = csv.writer(out_csv)\n",
    "    csv_writer.writerow(output_column_headings)\n",
    "    csv_writer.writerows(extracted_rows)\n",
    "\n",
    "print(\"The longitude and latitude locations of each profile have been extracted and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found within threshold for filtered_OMG_APEX_F9184_Dive_001.20200913T134432.science_log.csv\n",
      "No data found within threshold for filtered_OMG_APEX_F9184_Dive_002.20200914T102956.science_log.csv\n",
      "No data found within threshold for filtered_OMG_APEX_F9184_Dive_010.20200927T111350.science_log.csv\n",
      "No data found within threshold for filtered_OMG_APEX_F9184_Dive_016.20201014T201700.science_log.csv\n",
      "No data found within threshold for filtered_OMG_APEX_F9184_Dive_023.20201104T021208.science_log.csv\n",
      "No data found within threshold for filtered_OMG_APEX_F9184_Dive_025.20201109T215120.science_log.csv\n",
      "Data from OMG APEX F9184 sampled at {desired_depth} and saved to a CSV.\n"
     ]
    }
   ],
   "source": [
    "# Extract the profile data within 1 m of the desired depth. If there are multiple values within 1 m, the data provided at the closest depth to that desired is extracted.\n",
    "input_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/filtered_science_logs_depth/' # Define the directory of the CSV inputs\n",
    "save_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/' # Define the directory for the CSV\n",
    "\n",
    "# Read the csv file previously outputted containing the filename, lat and lon columns \n",
    "second_csv_file = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/f9184_lon_lat.csv'\n",
    "second_csv = pd.read_csv(second_csv_file)\n",
    "\n",
    "# Define the desired depth at which variables should be sampled, and the tolerance (1m)\n",
    "desired_depth = 240\n",
    "depth_tolerance = 1\n",
    "\n",
    "# Create a combined CSV with variables extracted at the value closest to the desired depth.\n",
    "dfs = []\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        input_filename = filename.replace('filtered_', '').replace('.csv', '')\n",
    "        prefix = input_filename.split('_')[0]\n",
    "        df = pd.read_csv(os.path.join(input_directory, filename))\n",
    "        depth_mask = (df['DEPTH'] >= (desired_depth - depth_tolerance)) & \\\n",
    "                     (df['DEPTH'] <= (desired_depth + depth_tolerance))\n",
    "        relevant_data = df[depth_mask]\n",
    "        \n",
    "        if not relevant_data.empty:\n",
    "            relevant_data = relevant_data.copy()  # Make a copy of the slice\n",
    "            relevant_data.loc[:, 'depth_diff'] = (relevant_data['DEPTH'] - desired_depth).abs()\n",
    "            closest_depth_row = relevant_data.loc[relevant_data['depth_diff'].idxmin()].copy() \n",
    "            closest_depth_row['filename'] = input_filename\n",
    "            dfs.append(closest_depth_row)\n",
    "        else:\n",
    "            print(f\"No data found within threshold for {filename}\")\n",
    "\n",
    "if dfs:\n",
    "    combined_data = pd.concat(dfs, axis=1).T\n",
    "    depth_str = str(desired_depth)\n",
    "    depth_tolerance_str = str(depth_tolerance)\n",
    "    combined_data = pd.merge(combined_data, second_csv[['FILENAME', 'LAT', 'LON']], left_on='filename', right_on='FILENAME', how='left')\n",
    "    combined_data = combined_data.drop(['depth_diff', 'FILENAME'], axis=1)\n",
    "    combined_data['DATE'] = pd.to_datetime(combined_data['TIME'], format='%Y%m%dT%H%M%S').dt.strftime('%d/%m/%Y')\n",
    "    column_order = ['filename', 'LAT', 'LON', 'MESSAGE', 'TIME', 'DATE', 'PRESSURE', 'TEMP', 'SALINITY', 'CONDUCTIVITY', 'INTERNAL_TEMP', 'DEPTH']\n",
    "    combined_data = combined_data[column_order]\n",
    "    output_file = os.path.join(save_directory, f'OMG_APEX_F9184_{depth_str}m_{depth_tolerance_str}m.csv')\n",
    "    combined_data.to_csv(output_file, index=False)\n",
    "else:\n",
    "    print(f\"No data found within {depth_tolerance} meters of the desired depth.\")\n",
    "\n",
    "print(\"Data from OMG APEX F9184 sampled at {desired_depth} and saved to a CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greenland Ecosystem Monitoring "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "velocity_download",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
