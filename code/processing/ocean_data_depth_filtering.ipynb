{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ocean Data Processing - Extract at a Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT REQUIRED MODULES\n",
    "import os\n",
    "import shutil\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import gsw # Gibbs-SeaWater (GSW) Oceanographic Toolbox \n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OMG ALAMO Floats (F9250 & F9313)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code processes the data recorded by two OMG ALAMO floats positioned within Disko Bay, F9250 & F9313 (https://podaac.jpl.nasa.gov/dataset/OMG_L1_FLOAT_ALAMO). The data are provided as JSON files - to begin, the number of JSON files provided within each directory is printed. The JSON files are then processed in order to sample oceanic variables at a specified depth. For the purposes of this study, oceanic variables were sampled at a depth of 240M within Disko Bay. Variables such as temperature, pressure and depth are extracted, with the date, longitude and latitude of each dive also recorded from the 'DiveStart' metadata. It should be noted that depth is not measured directly and is therefore calculated from pressure, using a package from the Gibbs-SeaWater (GSW) Oceanographic Toolbox (https://www.teos-10.org/software.htm#1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of JSON files in ALAMO_F9250 directory: 90\n",
      "Number of JSON files in ALAMO_F9313 directory: 94\n"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate the number of JSON files within a specified directory. \n",
    "def count_json_files(directory):\n",
    "    file_count = 0\n",
    "    for (dir_path, _, file_names) in os.walk(os.path.normpath(directory)):\n",
    "        for file in file_names:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_count += 1\n",
    "    return file_count\n",
    "\n",
    "ALAMO_F9250_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_alamo_f9250/'\n",
    "ALAMO_F9313_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_alamo_f9313/'\n",
    "print(f\"Number of JSON files in ALAMO_F9250 directory: {count_json_files(ALAMO_F9250_directory)}\")\n",
    "print(f\"Number of JSON files in ALAMO_F9313 directory: {count_json_files(ALAMO_F9313_directory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_alamo_f9250/ALAMO_F9250_240m.csv\n"
     ]
    }
   ],
   "source": [
    "# Specify the input directory, desired sampling depth and output filename.\n",
    "directory = ALAMO_F9250_directory\n",
    "desired_depth = 240\n",
    "output_filename = 'ALAMO_F9250_240m.csv'\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data.\n",
    "combined_df = pd.DataFrame(columns=['date', 'lon', 'lat','temperature', 'salinity', 'pressure', 'depth'])\n",
    "\n",
    "# Within the specified directory, open each JSON file in read mode. \n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        json_path = os.path.join(directory, filename)\n",
    "        with open(json_path, 'r') as openfile:\n",
    "            CTD_data = json.load(openfile)\n",
    "        CTD_data_dict = CTD_data[0]\n",
    "        \n",
    "        # Extract the longitude and latitude from the 'DiveStart'. If the JSON file doesn't have any lon or lat values, the file is skipped. \n",
    "        lat = CTD_data_dict.get('dives', [])[0].get('trajectory', {}).get('gps', [])[0].get('lat', None)\n",
    "        lon = CTD_data_dict.get('dives', [])[0].get('trajectory', {}).get('gps', [])[0].get('lon', None)\n",
    "        if lat == 0 or lon == 0:\n",
    "            continue\n",
    "\n",
    "        # Define the temperature, salinity and pressure keys within the CTD data dictionary.\n",
    "        temperature = CTD_data_dict.get('dives', [])[0].get('science', {}).get('ascending', {}).get('binned', {}).get('temperature', [])\n",
    "        salinity = CTD_data_dict.get('dives', [])[0].get('science', {}).get('ascending', {}).get('binned', {}).get('salinity', [])\n",
    "        pressure = np.array(CTD_data_dict.get('dives', [])[0].get('science', {}).get('ascending', {}).get('binned', {}).get('pressure', []))\n",
    "\n",
    "        # Check that the temperature and pressure variables are the same length (i.e. no missing values)\n",
    "        if len(temperature) == len(pressure) > 0:\n",
    "\n",
    "            # Calculate depth based on pressure, using the gsw package.\n",
    "            latitude = 69.2\n",
    "            depth = -1 * gsw.z_from_p(pressure, latitude)\n",
    "\n",
    "            # Create a dictionary with the keys for each desired variable. Convert this dictionary to a pandas DataFrame.\n",
    "            data_dict = {'temperature': temperature, 'salinity': salinity, 'pressure': pressure, 'depth': depth,\n",
    "                         'lon': [CTD_data_dict.get('dives', [])[0].get('trajectory', {}).get('gps', [])[0].get('lon', None)] * len(temperature),\n",
    "                         'lat': [CTD_data_dict.get('dives', [])[0].get('trajectory', {}).get('gps', [])[0].get('lat', None)] * len(temperature),\n",
    "                         'date': [CTD_data_dict.get('dives', [])[0].get('trajectory', {}).get('gps', [])[0].get('datetime', None)] * len(temperature)}\n",
    "            df = pd.DataFrame(data_dict)\n",
    "\n",
    "            # Extract the row of data sampled closest to the specified desired depth.\n",
    "            closest_row = df.iloc[(df['depth'] - desired_depth).abs().argsort()[:1]]\n",
    "            combined_df = pd.concat([combined_df, closest_row], ignore_index=True)\n",
    "\n",
    "# Convert the 'date' column to the desired format\n",
    "combined_df['date'] = pd.to_datetime(combined_df['date']).dt.strftime('%d/%m/%Y')\n",
    "output_csv = os.path.join(directory, output_filename)  # Use the predefined variable for the output filename\n",
    "\n",
    "# Save the combined DataFrame to an output CSV. \n",
    "combined_df.to_csv(output_csv, index=False)\n",
    "print(f\"Combined data saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OMG APEX Float (F9184)\n",
    "The following code processes the data recorded by OMG APEX float F9184 (https://podaac.jpl.nasa.gov/dataset/OMG_L1_FLOAT_APEX). This float was deployed in Disko Bay in September 2020. This profile was autonomous, with data from each dive saved as to a 'science log' csv. To begin, the science logs are filtered, with profiles only kept if pressure, temperature and salinity data were provided. Using the filtered CSVs, depth is then calculated based upon pressure, using the Gibbs Sea Water Oceanographic Toolbox module. The longitude and latitude of each profile is extracted using the first GPS coordinate recorded (i.e. the start location). Each profile is then sampled at the desired depth (240m), with a threshold of 1m used. If there are multiple values within 1 m, the data provided at the closest depth to that desired is extracted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CSV files have been filtered and saved.\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the science log csvs.\n",
    "input_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/science_logs/'\n",
    "\n",
    "# Define the directory to output the filtered csvs, containing temperature, pressure and salinity data.\n",
    "output_folder = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/filtered_science_logs/'\n",
    "\n",
    "# Rows to export from defined based on the 'message column (export those with at least temperature, pressure and salinity data.)\n",
    "target_strings = ['LGR_CP_PTS', 'LGR_CP_PTSC', 'LGR_CP_PTSCI']\n",
    "\n",
    "# Define the columns to label in each output csv.\n",
    "desired_columns = ['MESSAGE', 'TIME', 'PRESSURE', 'TEMP', 'SALINITY', 'CONDUCTIVITY', 'INTERNAL_TEMP']\n",
    "\n",
    "# LOOP THROUGH CSV FILES IN THE INPUT DIRECTORY \n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        input_filepath = os.path.join(input_directory, filename)\n",
    "        output_filename = f'filtered_{filename}'\n",
    "        output_filepath = os.path.join(output_folder, output_filename)\n",
    "        with open(input_filepath, 'r') as infile, open(output_filepath, 'w', newline='') as outfile:\n",
    "            reader = csv.reader(infile)\n",
    "            writer = csv.writer(outfile)\n",
    "            # Write the new column names\n",
    "            writer.writerow(desired_columns)\n",
    "            # Iterate through rows and write only if the first column matches the target strings\n",
    "            for row in reader:\n",
    "                if row and row[0] in target_strings:\n",
    "                    # Extract the values in the desired order\n",
    "                    filtered_row = [row[0], row[1], row[2], row[3], row[4], row[5], row[6]]\n",
    "                    writer.writerow(filtered_row)\n",
    "\n",
    "print(\"The CSV files have been filtered and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Depth has been calculated based on pressure. The updated CSVs have been saved.\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the filtered science log csvs.\n",
    "input_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/filtered_science_logs/'\n",
    "\n",
    "# Define the directory to output the filtered csvs, containing temperature, pressure and salinity data, with depth added.\n",
    "output_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/filtered_science_logs_depth/'\n",
    "\n",
    "# Latitude for Disko Bay (approximately 69.2 degrees North)\n",
    "latitude = 69.2\n",
    "\n",
    "# Loop through filtered CSV files in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.startswith('filtered_') and filename.endswith('.csv'):\n",
    "        input_filepath = os.path.join(input_directory, filename)\n",
    "        output_filepath = os.path.join(output_directory, filename)\n",
    "        with open(input_filepath, 'r') as infile, open(output_filepath, 'w', newline='') as outfile:\n",
    "            reader = csv.reader(infile)\n",
    "            writer = csv.writer(outfile)\n",
    "            header = next(reader)\n",
    "            header.append('DEPTH')\n",
    "            writer.writerow(header)\n",
    "            for row in reader:\n",
    "                message, time_str, pressure, temp, salinity, conductivity, internal_temp = row[:7]\n",
    "                pressure_dbar = float(pressure)  # Pressure should be in dbar\n",
    "                depth = gsw.z_from_p(pressure_dbar, latitude)\n",
    "                depth = -1 * depth\n",
    "                row.append(depth)\n",
    "                writer.writerow(row)\n",
    "\n",
    "print(\" Depth has been calculated based on pressure. The updated CSVs have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longitude and latitude locations of each profile have been extracted and saved.\n"
     ]
    }
   ],
   "source": [
    "# Using the original science logs, extract the longitude and latitude of each profile. This is extracted as the first GPS coordinate recorded (i.e. start location).\n",
    "original_folder_path = \"R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/science_logs/\" # Path to the folder containing the original CSV files\n",
    "output_file = \"R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/f9184_lon_lat.csv\" # Path for a csv containing the extracted longitude and latitude of each profile.\n",
    "extracted_rows = [] # Create an empty list to store the extracted rows\n",
    "output_column_headings = [\"FILENAME\", \"TIME\", \"LAT\", \"LON\"] # Define the column headings for the output CSV\n",
    "\n",
    "# Loop through each CSV file in the original folder\n",
    "for filename in os.listdir(original_folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        csv_file_path = os.path.join(original_folder_path, filename)\n",
    "        \n",
    "        with open(csv_file_path, 'r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            gps_found = False\n",
    "            \n",
    "            # Loop through rows of the CSV file\n",
    "            for row in csv_reader:\n",
    "                if row and row[0].strip().lower() == 'gps':\n",
    "                    if not gps_found:\n",
    "                        # Extract latitude and longitude data\n",
    "                        extracted_row = [filename] + row[1:-1]\n",
    "                        extracted_rows.append(extracted_row)\n",
    "                        gps_found = True\n",
    "                        break  # Stop searching after finding GPS data\n",
    "\n",
    "# Write the extracted data to the output CSV file\n",
    "with open(output_file, 'w', newline='') as out_csv:\n",
    "    csv_writer = csv.writer(out_csv)\n",
    "    csv_writer.writerow(output_column_headings)\n",
    "    csv_writer.writerows(extracted_rows)\n",
    "\n",
    "print(\"The longitude and latitude locations of each profile have been extracted and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found within threshold for filtered_OMG_APEX_F9184_Dive_001.20200913T134432.science_log.csv\n",
      "No data found within threshold for filtered_OMG_APEX_F9184_Dive_002.20200914T102956.science_log.csv\n",
      "No data found within threshold for filtered_OMG_APEX_F9184_Dive_010.20200927T111350.science_log.csv\n",
      "No data found within threshold for filtered_OMG_APEX_F9184_Dive_016.20201014T201700.science_log.csv\n",
      "No data found within threshold for filtered_OMG_APEX_F9184_Dive_023.20201104T021208.science_log.csv\n",
      "No data found within threshold for filtered_OMG_APEX_F9184_Dive_025.20201109T215120.science_log.csv\n",
      "Data from OMG APEX F9184 sampled at 240m and saved to a CSV.\n"
     ]
    }
   ],
   "source": [
    "# Extract the profile data within 1 m of the desired depth. If there are multiple values within 1 m, the data provided at the closest depth to that desired is extracted.\n",
    "input_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/filtered_science_logs_depth/' # Define the directory of the CSV inputs\n",
    "save_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/' # Define the directory for the CSV\n",
    "\n",
    "# Define the desired depth at which variables should be sampled, and the tolerance (1m)\n",
    "desired_depth = 240\n",
    "depth_tolerance = 1\n",
    "\n",
    "# Create a combined CSV with variables extracted at the value closest to the desired depth.\n",
    "dfs = []\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        input_filename = filename.replace('filtered_', '').replace('.csv', '')\n",
    "        prefix = input_filename.split('_')[0]\n",
    "        df = pd.read_csv(os.path.join(input_directory, filename))\n",
    "        depth_mask = (df['DEPTH'] >= (desired_depth - depth_tolerance)) & \\\n",
    "                     (df['DEPTH'] <= (desired_depth + depth_tolerance))\n",
    "        relevant_data = df[depth_mask]\n",
    "        \n",
    "        if not relevant_data.empty:\n",
    "            relevant_data = relevant_data.copy()\n",
    "            relevant_data.loc[:, 'depth_diff'] = (relevant_data['DEPTH'] - desired_depth).abs()\n",
    "            closest_depth_row = relevant_data.loc[relevant_data['depth_diff'].idxmin()].copy() \n",
    "            closest_depth_row['filename'] = filename\n",
    "            dfs.append(closest_depth_row)\n",
    "        else:\n",
    "            print(f\"No data found within threshold for {filename}\")\n",
    "\n",
    "if dfs:\n",
    "    combined_data = pd.concat(dfs, axis=1).T\n",
    "    depth_str = str(desired_depth)\n",
    "    depth_tolerance_str = str(depth_tolerance)\n",
    "    combined_data = combined_data.drop(['depth_diff'], axis=1)\n",
    "    combined_data['DATE'] = pd.to_datetime(combined_data['TIME'], format='%Y%m%dT%H%M%S').dt.strftime('%d/%m/%Y')\n",
    "    column_order = ['filename', 'MESSAGE', 'TIME', 'DATE', 'PRESSURE', 'TEMP', 'SALINITY', 'CONDUCTIVITY', 'INTERNAL_TEMP', 'DEPTH']\n",
    "    combined_data = combined_data[column_order]\n",
    "    output_file = os.path.join(save_directory, f'OMG_APEX_F9184_{depth_str}m_{depth_tolerance_str}m.csv')\n",
    "    combined_data.to_csv(output_file, index=False)\n",
    "else:\n",
    "    print(f\"No data found within {depth_tolerance} meters of the desired depth.\")\n",
    "\n",
    "print(f\"Data from OMG APEX F9184 sampled at {desired_depth}m and saved to a CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitude and latitude data added.\n"
     ]
    }
   ],
   "source": [
    "# Update the csv with the latitude and longitude values for each profile sampled at 240m. \n",
    "profile_csv = pd.read_csv('R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/OMG_APEX_F9184_240m_1m.csv')\n",
    "lon_lat_csv = pd.read_csv('R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/f9184_lon_lat.csv')\n",
    "\n",
    "profile_csv['FILENAME'] = profile_csv['filename'].apply(lambda x: x.replace('filtered_', '')) # Remove the 'filtered_' prefix from the filename column in the omg_apex data\n",
    "merged_data = pd.merge(profile_csv, lon_lat_csv[['FILENAME', 'LAT', 'LON']], on='FILENAME', how='left') # Merge longitude and latitude data with the existing data\n",
    "merged_data = merged_data.drop(['FILENAME'], axis=1) # Drop redundant columns\n",
    "merged_data.to_csv('R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/omg_apex_f9184/OMG_APEX_F9184_240m_1m.csv', index=False) # Overwrite the existing CSV file with the updated data\n",
    "\n",
    "print(f\"Longitude and latitude data added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greenland Ecosystem Monitoring \n",
    "The GEM Database (https://data.g-e-m.dk/log-in/data-tables/) was searched using ‘MarineBasis Disko’ - ‘Water Column’ - ‘CTD Measurments’ - ‘2018-01-01 to 2022-12-31’, with the data available then downloaded and saved as 'GEM_download'. To begin, the combined CSV was split into individual profiles, based on the 'Date'. All profiles that reached a depth of 240 m were then copied to the '240M_profiles' folder. These profiles were then sampled at the desired depth (240m), with a threshold of 1m used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual profiles have been exported from the combined download, using the date column.\n"
     ]
    }
   ],
   "source": [
    "# Open the 'GEM_download' csv and export each profile, defined by data, to an individual csv. \n",
    "input_file = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/gem_ctd_disko_bay/GEM_download.csv'\n",
    "df = pd.read_csv(input_file, delimiter='\\t')\n",
    "grouped = df.groupby('Date')\n",
    "output_folder = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/gem_ctd_disko_bay/individual_profiles/'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "for date, group in grouped:\n",
    "    formatted_date = date.replace('/', '-')\n",
    "    output_folder_date = os.path.join(output_folder, f'GEM_CSV_profile_{formatted_date}')\n",
    "    output_filename = os.path.join(output_folder, f'GEM_CSV_profile_{formatted_date}.csv')\n",
    "    group.to_csv(output_filename, index=False)\n",
    "    \n",
    "print(f\"Individual profiles have been exported from the combined download, using the date column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CSV files that reach a depth of at least 240m have been copied to: R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/gem_ctd_disko_bay/240M_profiles/\n"
     ]
    }
   ],
   "source": [
    "# Copy all profiles that reached a depth of 240 to the '240M_profiles' folder.\n",
    "input_folder = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/gem_ctd_disko_bay/individual_profiles/'\n",
    "output_folder = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/gem_ctd_disko_bay/240M_profiles/'\n",
    "csv_files = [file for file in os.listdir(input_folder) if file.endswith('.csv')]\n",
    "for csv_file in csv_files:\n",
    "    csv_path = os.path.join(input_folder, csv_file)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if (df['Depth (m)'] >= 240).any():\n",
    "        shutil.copy(csv_path, output_folder)\n",
    "\n",
    "print(f\"The CSV files that reach a depth of at least 240m have been copied to: {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from the GEM sampled at the closest depth to 240m with a tolerance of 1m and saved to a CSV.\n"
     ]
    }
   ],
   "source": [
    "# Define input and output directories\n",
    "input_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/gem_ctd_disko_bay/240M_profiles/'\n",
    "save_directory = 'R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/gem_ctd_disko_bay/'\n",
    "\n",
    "# Define the desired depth at which variables should be sampled, and the tolerance (1m)\n",
    "desired_depth = 240\n",
    "depth_tolerance = 1\n",
    "\n",
    "# Create a combined CSV with variables extracted at the value closest to the desired depth.\n",
    "dfs = []\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(input_directory, filename))\n",
    "        depth_mask = (df['Depth (m)'] >= desired_depth - depth_tolerance) & (df['Depth (m)'] <= desired_depth + depth_tolerance)\n",
    "        relevant_data = df[depth_mask]\n",
    "        if not relevant_data.empty:\n",
    "            relevant_data = relevant_data.copy()\n",
    "            closest_depth_row = relevant_data.iloc[(relevant_data['Depth (m)'] - desired_depth).abs().argsort()[:1]]\n",
    "            closest_depth_row = closest_depth_row[['Date', 'Latitude', 'Longitude', 'Depth (m)', 'Temp (°C)', 'Salinity (PSU)']]\n",
    "            dfs.append(closest_depth_row)\n",
    "        else:\n",
    "            print(f\"No data found within threshold for {filename}\")\n",
    "\n",
    "if dfs:\n",
    "    combined_data = pd.concat(dfs, ignore_index=True)\n",
    "    output_file = os.path.join(save_directory, f'GEM_{desired_depth}m.csv')\n",
    "    combined_data.to_csv(output_file, index=False)\n",
    "else:\n",
    "    print(f\"No data found within {depth_tolerance} meters of the desired depth.\")\n",
    "\n",
    "print(f\"Data from the GEM sampled at the closest depth to {desired_depth}m with a tolerance of {depth_tolerance}m and saved to a CSV.\")\n",
    "\n",
    "# Alter the format of the longitude and latitude columns so each profile can be plotted in QGIS.\n",
    "sampled_240m = pd.read_csv('R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/gem_ctd_disko_bay/GEM_240m.csv')\n",
    "\n",
    "# Function to convert degrees and minutes to decimal degrees\n",
    "def convert_to_decimal(coord):\n",
    "    degrees, minutes, direction = coord.replace('°', '').split()\n",
    "    sign = -1 if direction in ['S', 'W'] else 1\n",
    "    return sign * (int(degrees) + float(minutes) / 60)\n",
    "\n",
    "# Convert latitude and longitude to decimal degrees\n",
    "sampled_240m['Latitude (Decimal Degrees)'] = sampled_240m['Latitude'].apply(lambda x: convert_to_decimal(x))\n",
    "sampled_240m['Longitude (Decimal Degrees)'] = sampled_240m['Longitude'].apply(lambda x: convert_to_decimal(x))\n",
    "sampled_240m.drop(columns=['Latitude', 'Longitude'], inplace=True)\n",
    "\n",
    "# Save the updated DataFrame back to CSV\n",
    "sampled_240m.to_csv('R:/JAKOBSHAVN/CODE/github/jakobshavn_isbrae/data/gem_ctd_disko_bay/GEM_240m.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "velocity_download",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
